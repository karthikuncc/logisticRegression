{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd #pandas is a dataframe library\n",
    "import matplotlib.pyplot as plt  #matplotlib.pyplot plots data\n",
    "import numpy as np  #numpy provides n-dimension object support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from numpy import loadtxt, where\n",
    "from pylab import scatter, show, legend, xlabel, ylabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:/Users/scoda/Desktop/CreditRiskAnalysis/Training50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.00% in training set\n",
      "30.00% in test set\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "feature_col_names=['Account.Balance','Duration.of.Credit..month.','Payment.Status.of.Previous.Credit','Purpose','Credit.Amount',\n",
    "                  'Value.Savings.Stocks','Length.of.current.employment','Instalment.per.cent','Sex...Marital.Status',\n",
    "                   'Guarantors','Duration.in.Current.address','Most.valuable.available.asset','Age..years.','Concurrent.Credits',\n",
    "                   'Type.of.apartment','No.of.Credits.at.this.Bank','No.of.dependents','Telephone','Foreign.Worker' \n",
    "                  ]\n",
    "predicted_class_names = ['Creditability']\n",
    "\n",
    "\n",
    "#X=df[feature_col_names].values  #predictor feature columns\n",
    "X = np.array(X)\n",
    "y=df[predicted_class_names].values  #predicted Class (1=true ,0=false) column(1 x m)\n",
    "Y = np.array(y)\n",
    "split_test_size=0.30\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=split_test_size,random_state=42)\n",
    "#test_size=0.3 is 30% \n",
    "\n",
    "print(\"{0:2.2f}% in training set\".format((len(X_train)/len(df.index))*100))\n",
    "print(\"{0:2.2f}% in test set\".format((len(X_test)/len(df.index))*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score Scikit learn:  0.746666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scoda\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# train scikit learn model \n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,Y_train)\n",
    "print('score Scikit learn: ', clf.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##The sigmoid function adjusts the cost function hypotheses to adjust the algorithm proportionally for worse estimations\n",
    "def Sigmoid(z):\n",
    "\tG_of_Z = float(1.0 / float((1.0 + math.exp(-1.0*z))))\n",
    "\treturn G_of_Z \n",
    "\n",
    "##The hypothesis is the linear combination of all the known factors x[i] and their current estimated coefficients theta[i] \n",
    "##This hypothesis will be used to calculate each instance of the Cost Function\n",
    "def Hypothesis(theta, x):\n",
    "\tz = 0\n",
    "\tfor i in range(0,len(theta)-1):\n",
    "\t\t#print(i)\n",
    "\t\tif(len(x)>len(theta)):\n",
    "\t\t\tz += x[i]*theta[i]\n",
    "\treturn Sigmoid(z)\n",
    "\n",
    "##For each member of the dataset, the result (Y) determines which variation of the cost function is used\n",
    "##The Y = 0 cost function punishes high probability estimations, and the Y = 1 it punishes low scores\n",
    "##The \"punishment\" makes the change in the gradient of ThetaCurrent - Average(CostFunction(Dataset)) greater\n",
    "def Cost_Function(X,Y,theta,m):\n",
    "\tsumOfErrors = 0\n",
    "\tfor i in range(0,m):\n",
    "\t\txi = X[i]\n",
    "\t\thi = Hypothesis(theta,xi)\n",
    "\t\t\n",
    "\t\tif Y[i] == 1:\n",
    "\t\t\terror = Y[i] * math.log(hi)\n",
    "\t\telif Y[i] == 0 and hi!=1:\n",
    "\t\t\terror = (1-Y[i]) * math.log(1-hi)\n",
    "\t\tsumOfErrors += error\n",
    "\tconst = -1/m\n",
    "\tJ = const * sumOfErrors\n",
    "\tprint('cost is ', J)\n",
    "\treturn J\n",
    "##This function creates the gradient component for each Theta value \n",
    "##The gradient is the partial derivative by Theta of the current value of theta minus \n",
    "##a \"learning speed factor aplha\" times the average of all the cost functions for that theta\n",
    "##For each Theta there is a cost function calculated for each member of the dataset\n",
    "def Cost_Function_Derivative(X,Y,theta,j,m,alpha):\n",
    "\tsumErrors = 0\n",
    "\tfor i in range(0,m):\n",
    "\t\txi = X[i]\n",
    "\t\txij = xi[j]\n",
    "\t\thi = Hypothesis(theta,X[i])\n",
    "\t\terror = (hi - Y[i])*xij\n",
    "\t\tsumErrors += error\n",
    "\tm = len(Y)\n",
    "\tconstant = float(alpha)/float(m)\n",
    "\tJ = constant * sumErrors\n",
    "\treturn J\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##For each theta, the partial differential \n",
    "##The gradient, or vector from the current point in Theta-space (each theta value is its own dimension) to the more accurate point, \n",
    "##is the vector with each dimensional component being the partial differential for each theta value\n",
    "def Gradient_Descent(X,Y,theta,m,alpha):\n",
    "\tnew_theta = []\n",
    "\tconstant = alpha/m\n",
    "\tfor j in range(0,len(theta)):\n",
    "\t\tCFDerivative = Cost_Function_Derivative(X,Y,theta,j,m,alpha)\n",
    "\t\tnew_theta_value = theta[j] - CFDerivative\n",
    "\t\tnew_theta.append(new_theta_value)\n",
    "\treturn new_theta\n",
    "\n",
    "##The high level function for the LR algorithm which, for a number of steps (num_iters) finds gradients which take \n",
    "##the Theta values (coefficients of known factors) from an estimation closer (new_theta) to their \"optimum estimation\" which is the\n",
    "##set of values best representing the system in a linear combination model\n",
    "def Logistic_Regression(X,Y,alpha,theta,num_iters):\n",
    "\tm = len(Y)\n",
    "\tfor x in range(1,num_iters):\n",
    "\t\tnew_theta = Gradient_Descent(X,Y,theta,m,alpha)\n",
    "\t\ttheta = new_theta\n",
    "\t\tif x % 100 == 0:\n",
    "\t\t\t#here the cost function is used to present the final hypothesis of the model in the same form for each gradient-step iteration\n",
    "\t\t\tCost_Function(X,Y,theta,m)\n",
    "\t\t\tprint('theta ', theta)\t\n",
    "\t\t\tprint('cost is ', Cost_Function(X,Y,theta,m))\n",
    "    \n",
    "\tDeclare_Winner(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost is  [ 0.61672274]\n",
      "theta  [array([ 0.79255038]), array([-1.06624872])]\n",
      "cost is  [ 0.61672274]\n",
      "cost is  [ 0.61672274]\n",
      "cost is  [ 0.61360668]\n",
      "theta  [array([ 0.95593999]), array([-2.02597052])]\n",
      "cost is  [ 0.61360668]\n",
      "cost is  [ 0.61360668]\n",
      "cost is  [ 0.613436]\n",
      "theta  [array([ 0.99438994]), array([-2.96453805])]\n",
      "cost is  [ 0.613436]\n",
      "cost is  [ 0.613436]\n",
      "cost is  [ 0.6134259]\n",
      "theta  [array([ 1.00375405]), array([-3.89818089])]\n",
      "cost is  [ 0.6134259]\n",
      "cost is  [ 0.6134259]\n",
      "cost is  [ 0.61342529]\n",
      "theta  [array([ 1.00605385]), array([-4.83062763])]\n",
      "cost is  [ 0.61342529]\n",
      "cost is  [ 0.61342529]\n",
      "cost is  [ 0.61342526]\n",
      "theta  [array([ 1.00661984]), array([-5.7627808])]\n",
      "cost is  [ 0.61342526]\n",
      "cost is  [ 0.61342526]\n",
      "cost is  [ 0.61342526]\n",
      "theta  [array([ 1.00675921]), array([-6.69486174])]\n",
      "cost is  [ 0.61342526]\n",
      "cost is  [ 0.61342526]\n",
      "cost is  [ 0.61342526]\n",
      "theta  [array([ 1.00679353]), array([-7.6269249])]\n",
      "cost is  [ 0.61342526]\n",
      "cost is  [ 0.61342526]\n",
      "cost is  [ 0.61342526]\n",
      "theta  [array([ 1.00680198]), array([-8.55898367])]\n",
      "cost is  [ 0.61342526]\n",
      "cost is  [ 0.61342526]\n",
      "Scikit won.\n",
      "Your score:  0.31333333333333335\n",
      "Scikits score:  0.693333333333\n"
     ]
    }
   ],
   "source": [
    "##This method compares the accuracy of the model generated by the scikit library with the model generated by this implementation\n",
    "def Declare_Winner(theta):\n",
    "    score = 0\n",
    "    winner = \"\"\n",
    "    #first scikit LR is tested for each independent var in the dataset and its prediction is compared against the dependent var\n",
    "    #if the prediction is the same as the dataset measured value it counts as a point for thie scikit version of LR\n",
    "    scikit_score = clf.score(X_test,Y_test)\n",
    "    length = len(X_test)\n",
    "    for i in range(0,length):\n",
    "        prediction = round(Hypothesis(X_test[i],theta))\n",
    "        answer = Y_test[i]\n",
    "        if prediction == answer:\n",
    "            score += 1\n",
    "    #the same process is repeated for the implementation from this module and the scores compared to find the higher match-rate\n",
    "    my_score = float(score) / float(length)\n",
    "    if my_score > scikit_score:\n",
    "        print('You won!')\n",
    "    elif my_score == scikit_score:\n",
    "        print('Its a tie!')\n",
    "    else:\n",
    "        print('Scikit won.')\n",
    "    print('Your score: ', my_score)\n",
    "    print('Scikits score: ', scikit_score) \n",
    "\n",
    "# These are the initial guesses for theta as well as the learning rate of the algorithm\n",
    "# A learning rate too low will not close in on the most accurate values within a reasonable number of iterations\n",
    "# An alpha too high might overshoot the accurate values or cause irratic guesses\n",
    "# Each iteration increases model accuracy but with diminishing returns, \n",
    "# and takes a signficicant coefficient times O(n)*|Theta|, n = dataset length\n",
    "initial_theta = [0,0]\n",
    "alpha = 0.1\n",
    "iterations = 1000\n",
    "Logistic_Regression(X,Y,alpha,initial_theta,iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
